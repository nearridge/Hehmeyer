---
title: "EDA"
author: "Neeraj Sharma"
date: "10/1/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(data.table)
library(magrittr)
library(gghighlight)
library(lubridate)
library(broom)
library(glue)
library(knitr)
library(fitdistrplus)

data <- read_csv("../funding_data.csv")
```

# KEY FINDINGS

The median duration of deviation for the funding rate data is 3 for both above and below. However, the mean length of deviation is considerably higher for positive swings; 5.8 versus 4.9. Furthermore, there appear to be considerably more periods of negative funding than positve funding (291 vs 199); a difference of 92 periods. This implies that the price of swaps  exceeds the spot price of bitcoin more than the converse.

In terms of projecting the cumulative returns to funding, it appears that positive funding periods return more than negative funding periods. The coefficient for time in the positive funding regression is 0.0019147 while the coefficient for the negative funding regression is -0.0013052. Furthermore, the median cumulative funding return is considerably higher for positive funding periods compared to negative funding periods. 

Finally, all things considered, I find that funding periods are generally extremely short lived. Based on visual analysis of the distributions of funding history lengths, funding is likely to last only for 1 period. However, by modeling using a distribution function which likely provides a closer upper bound on the true likelihood indicates that positive funding tends to last longer than negative funding. 

## When the funding rate deviates from +1bps either below or above, how long did it take for the funding rate to revert back to +1bps when it was below and how long did it take when it was above.

I produce a dataframe that has signals for whenever we are +/- 0.0001 basis points and then count within these periods to get the length of each period of deviation. This is a sample of the dataframe I produce. Signal indicates if you are positive (1), negative (-1), or neutral (0). Section counts increases every time signal changes, meaning it counts the unique number of meaningful periods of funding. CSum is the cumulative sum within a period of significant funding. ID counts the position within a period of significant funding. 


```{r}
running_size <- data %>%
  filter(timestamp > ymd_hms("2017-06-05 04:00:00", tz = "US/Eastern")) %>%
  mutate(signal = if_else(fundingRate > 0.0001, 1, if_else(fundingRate < -0.0001, -1, 0)),
         section = rleid(signal)) %>%
  group_by(section) %>%
  mutate(csum = cumsum(fundingRate),
         csum = if_else(signal == 0, 0, csum),
         id = row_number(),
         id = ifelse(signal == 0, NA, id)) %>%
  arrange(desc(row_number())) %>%
  ungroup()

running_size %>%
  head(11) %>%
  kable()
```

Here are a couple of visualization that examine this more closely.

```{r}
running_size %>%
  group_by(signal, section) %>%
  filter(signal != 0) %>%
  summarize(max = max(id)) %>%
  ggplot(aes(x = max)) + 
  geom_histogram(color = "black") + 
  labs(title = "Histogram of Duration of Funding Periods (-1bps/+1bps)") + 
  facet_wrap(~signal, scales = "free_x")



#How should I model this?
running_size %>%
  filter(signal == 1) %>%
  ggplot() + 
  geom_line(mapping = aes(x = id, y = fundingRate, group = section), alpha = 0.3) + 
  geom_smooth(mapping = aes(x = id, y = fundingRate)) +
  labs(title = "Reversion tendencies of funding periods (+)", x = "Number of funding intervals")

running_size %>%
  filter(signal == -1) %>%
  ggplot() + 
  geom_line(mapping = aes(x = id, y = fundingRate, group = section), alpha = 0.3) + 
  geom_smooth(mapping = aes(x = id, y = fundingRate)) +
  labs(title = "Reversion tendencies of funding periods (-)", x = "Number of funding intervals")
```

Here are some of the summary stats on reversion in each direction

```{r}
running_size %>%
  filter(signal != 0) %>%
  group_by(signal) %>%
  summarize(mean = mean(id),
            med = median(id),
            stdev = sd(id)) %>% right_join(
              running_size %>%
                group_by(signal, section) %>%
                filter(id == max(id)) %>%
                ungroup() %>%
                group_by(signal) %>%
                summarize(`number of unique periods` = n())
            ) %>%
  kable()
```

## What is the cumulative funding rate? Sum of the funding rates when cumulative days above or below 1?

Aggregate funding rate throughout all history. 

```{r}
running_size %>% 
  arrange(timestamp) %>% 
  mutate(cumsum = cumsum(fundingRate)) %>%
  ggplot() +
  geom_line(aes(x = timestamp, y = cumsum)) + 
  labs(title = "Aggregate running cumulative sum of funding rate")
```

```{r}
ggplot(running_size) + 
  geom_line(aes(x = timestamp, y = csum)) + 
  labs(title = "Cumulative Sum of Funding Rates ONLY within Significant Funding Periods", x = "Time", y = "Returns", color = "") +
  theme(legend.position = "bottom")
```

```{r}
running_size %>%
  filter(signal == 1) %>%
  ggplot() + 
  geom_line(mapping = aes(x = id, y = csum, group = section)) + 
  geom_smooth(aes(x = id, y = csum), method = "lm") + 
  gghighlight(max(csum) > 0.03, label_key = section, unhighlighted_params = list(alpha = 1)) +
  labs(title = "Cumulative Sum of Funding Rate in Significant Periods (+)", subtitle = "Normed to uniform start index", x = "Number of funding intervals")

lm(csum ~ id, data = running_size %>% filter(signal == 1)) %>%
  tidy() %>%
  kable()

running_size %>%
  filter(signal == -1) %>%
  ggplot() + 
  geom_line(mapping = aes(x = id, y = csum, group = section)) + 
  geom_smooth(aes(x = id, y = csum), method = "lm") + 
  gghighlight(min(csum) < -0.03, label_key = section, unhighlighted_params = list(alpha = 1)) +
  labs(title = "Cumulative Sum of Funding Rate in Significant Periods (+)", subtitle = "normed to uniform start index", x = "Number of funding intervals")

lm(csum ~ id, data = running_size %>% filter(signal == -1)) %>%
  tidy() %>%
  kable()
```

Summary stats of cumulative returns in terms of value of the cumulative return. 

```{r}
running_size %>%
  filter(signal != 0) %>%
  group_by(signal) %>%
  summarize(mean = mean(csum), min = min(csum), med = median(csum), max = max(csum), stdev = sd(csum)) %>%
  kable()
```

## 3. What is the likelihood of funding rate deviating from 1 again in the next 1 period

I want P(FR_t+1 > 0.0001 | FR_t > 0.0001). Alternatively, fit a poisson distribution, but the issue here is that we are not independently generated. If a event occured in t, it is likely it occured in t+1 it seems based on investor behavior. My research says that modeling these events with a poisson distribution yields an upper-bound approximation for the true distribution. That is still insightful, given the distribution of the data clearly lends itself to 1 period being the highest likelyhood of deviation duration. 

```{r}
running_size %>%
  filter(signal == 1) %>%
  group_by(id) %>%
  summarize(count = n()) %>%
  mutate(prob = count / sum(count)) %>%
  bind_cols(dpois(c(1:36), mean(
    running_size %>%
      filter(signal == 1) %>%
      pull(id)
  ))) %>%
  ggplot(aes(x = id, y = prob)) +
  geom_col() +
  geom_text(aes(label = round(prob, 3), y = prob + 0.01), angle = 45) +
  geom_line(aes(x = id, y = `...4`)) +
  geom_vline(xintercept = mean(
    running_size %>%
      filter(signal == 1) %>%
      pull(id)
  )) + 
  annotate("text", x = mean(
    running_size %>%
      filter(signal == 1) %>%
      pull(id)
  ), y = 0.2, label = glue("lambda = \n", mean(
    running_size %>%
      filter(signal == 1) %>%
      pull(id)
  ) %>% round(3))) + 
  labs(title = "distribution of length of significant period with upper-bound PMF overlaid (+)", subtitle = "probability that you're currently in the last period. Likelyhood you will deviate again is 1-p")

running_size %>%
  filter(signal == -1) %>%
  group_by(id) %>%
  summarize(count = n()) %>%
  mutate(prob = count / sum(count)) %>%
  bind_cols(dpois(c(1:22), mean(
    running_size %>%
      filter(signal == -1) %>%
      pull(id)
  ))) %>%
  ggplot(aes(x = id, y = prob)) +
  geom_col() +
  geom_text(aes(label = round(prob, 3), y = prob + 0.01), angle = 45) +
  geom_line(aes(x = id, y = `...4`)) +
  geom_vline(xintercept = mean(
    running_size %>%
      filter(signal == -1) %>%
      pull(id)
  )) + 
  annotate("text", x = mean(
    running_size %>%
      filter(signal == -1) %>%
      pull(id)
  ), y = 0.27, label = glue("lambda = \n", mean(
    running_size %>%
      filter(signal == -1) %>%
      pull(id)
  ) %>% round(3))) + 
  labs(title = "distribution of length of significant period with upper-bound PMF overlaid (-)", subtitle = "probability that you're currently in the last period. Likelyhood you will deviate again is 1-p")
```

To get a middle and lower bound, your answer is just 1 by gut checking the data above. 

## Other random exploration

```{r}
data %>%
  mutate(above = if_else(fundingRate > 0.0001, 1, 
                        if_else(fundingRate < -0.0001, -1, 0))) %>%
  ggplot(aes(x = timestamp, y = fundingRate)) + 
  geom_line() + 
  geom_hline(yintercept = -0.0001) + 
  geom_hline(yintercept = 0.0001) + 
  labs(title = "Lifetime Funding Rate Data Plotted") 
```

```{r}
ytd2019 <- data %>%
  filter(timestamp < ymd_hms("2020-01-01 00:00:00", tz = "US/Eastern"),
         timestamp >= ymd_hms("2019-01-01 00:00:00", tz = "US/Eastern"))

ar <- arima(ytd2019$fundingRate, order = c(3, 0, 15))
resids <- residuals(ar)

fit <- ytd2019$fundingRate - resids

fitted <- tibble(timestamp = ytd2019$timestamp, values = fit)

ggplot() + 
  geom_line(ytd2019, mapping = aes(timestamp, fundingRate)) + 
  geom_line(fitted, mapping = aes(timestamp, values), color = "green") + 
  labs(title = "Historic Funding Data for 2019 with Model Fitted")
```

Lets see how the model fairs out of sample

```{r}
abbv_ytd2020 <- data %>%
  filter(timestamp <= ymd_hms("2020-01-07 20:00:00", tz = "US/Eastern"),
         timestamp >= ymd_hms("2020-01-01 00:00:00", tz = "US/Eastern"))


prediction <- predict(ar, n.ahead = 20)
forecasted <- tibble(timestamp = abbv_ytd2020$timestamp[1:20], pred = prediction[["pred"]] %>% as.numeric())

ggplot() + 
  geom_line(bind_rows(ytd2019 %>% filter(timestamp >= ymd_hms("2019-12-17 04:00:00", tz = "US/Eastern")), abbv_ytd2020), mapping = aes(timestamp, fundingRate)) + 
  geom_line(fitted %>% filter(timestamp >= ymd_hms("2019-12-17 04:00:00", tz = "US/Eastern")), mapping = aes(timestamp, values), color = "green") + 
  geom_line(forecasted, mapping = aes(timestamp, pred), color = "red") + 
  labs(title = "Zoomed in on Out of Sample prediction", subtitle = "This is not interesting because I don't have volatility. Need to pick a better sample.")
```

Let me try this attempt to predict right at the beginning of a period of volatility

```{r}
ytd2020 <- data %>%
  filter(timestamp >= ymd_hms("2020-01-01 00:00:00", tz = "US/Eastern"))

ggplot(ytd2020, aes(timestamp, fundingRate)) + 
  geom_line() + 
  labs(title = "Historic Funding Rate for 2020", subtitle = "Lets specifically analyze the period beginning on 2020-09-07 04:00:00") + 
  annotate("text", x = ymd_hms("2020-09-07 04:00:00"), y = -0.001, label = "2020-09-07 04:00:00") + 
  annotate("point", x = ymd_hms("2020-09-07 04:00:00"), y = -0.00075)
```

Here is a model that is fit up until 2020-09-07 04:00:00. We will use this to predict the next 40 funding periods and see what happens. 

```{r}
training <- ytd2020 %>%
  filter(timestamp < ymd_hms("2020-09-07 04:00:00"))

testing <- ytd2020 %>%
  filter(timestamp >= ymd_hms("2020-09-07 04:00:00"))

# Lets fit some models
acf(training$fundingRate)
pacf(training$fundingRate)
```


